# ğŸ“„ AI PDF Chat Application (Backend â€“ Phase 1 to Phase 5)

A production-grade backend for an AI-powered PDF chat application, designed with real-world scalability and clean architecture in mind. This project currently implements secure authentication, PDF ingestion, background processing, text chunking, and vector embeddings stored using pgvector.

## ğŸš€ Project Status

- âœ… **Phase 1** â€“ Core Backend & Authentication
- âœ… **Phase 2** â€“ Background Queue (BullMQ + Redis)
- âœ… **Phase 3** â€“ PDF Parsing & Text Chunking
- âœ… **Phase 4** â€“ Database Storage (Documents + Chunks)
- âœ… **Phase 5** â€“ Vector Embeddings with pgvector (Gemini)
- â³ **Phase 6** â€“ Chat with PDF (Upcoming)

## ğŸ§  What This Project Demonstrates

- Secure JWT-based authentication using Clerk
- File upload handling with Multer
- Asynchronous background job processing using BullMQ + Redis
- PDF text extraction and intelligent chunking
- Vector embeddings generation using Google Gemini
- Semantic search-ready storage using PostgreSQL + pgvector
- Clean separation between API, queue, worker, and utility layers
- Industry-style backend architecture for RAG systems

## ğŸ—ï¸ Tech Stack

### Backend
- Node.js
- Express.js
- PostgreSQL
- pgvector
- Redis
- BullMQ

### Authentication
- Clerk (JWT-based authentication)

### File Handling
- Multer (local temporary storage)

### AI / Embeddings
- Google Gemini (`text-embedding-004`)

## ğŸ” Authentication Flow (Clerk)

1. User authenticates on the frontend using Clerk
2. Frontend sends JWT in the `Authorization` header
3. Backend verifies the token using Clerk middleware
4. Only authenticated users can upload PDFs

âŒ Unauthenticated requests return `401 Unauthorized`

## ğŸ“¤ File Upload & Ingestion Flow

1. Authenticated user uploads a PDF
2. File is temporarily stored in `/uploads`
3. File metadata is saved in PostgreSQL (`documents` table)
4. A background job is added to BullMQ
5. API responds instantly (non-blocking)

## âš™ï¸ Background Processing (BullMQ + Redis)

- Redis acts as a job broker
- BullMQ manages asynchronous job execution
- A dedicated worker processes PDF ingestion

### Example Worker Logs

```
ğŸ“„ Started processing documentId
ğŸ§  Extracted text from PDF
ğŸ“¦ Stored chunks in database
ğŸ§  Generated embeddings
ğŸ‰ Document fully processed
```

âœ” Upload API remains fast  
âœ” Heavy processing runs asynchronously

## ğŸ§  PDF Processing Pipeline (Phase 3â€“5)

1. PDF is parsed inside a BullMQ worker
2. Extracted text is split into semantic chunks
3. Each chunk is stored in `document_chunks`
4. Gemini generates embeddings for each chunk
5. Embeddings are stored using pgvector (`vector(768)`)

## ğŸ—„ï¸ Database Schema

### `documents`
Stores metadata about uploaded PDFs.

- `id`
- `user_id`
- `original_name`
- `file_name`
- `file_path`
- `file_size`
- `status`
- `created_at`

### `document_chunks`
Stores chunked text and vector embeddings.

- `id`
- `document_id`
- `chunk_index`
- `content`
- `embedding` (pgvector â€“ 768 dimensions)

### `chat_history`
Prepared for Phase 6 (AI chat).

## ğŸ›‘ Important Notes

- `uploads/` is ignored by Git
- `.env` is never committed
- Local file storage is temporary
- Production setup will use object storage (e.g., S3)
- Embeddings are generated asynchronously to avoid API blocking

## ğŸ¯ What's Next (Phase 6)

- Embed user queries
- Perform pgvector similarity search
- Retrieve top-k relevant chunks
- Generate grounded answers using Gemini
- Prevent hallucinations via context-only prompting

## ğŸ“Œ Summary

This backend forms a complete ingestion and vectorization pipeline for a Retrieval-Augmented Generation (RAG) system and is ready for semantic search and AI-powered document chat.